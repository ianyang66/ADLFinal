{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "https://github.com/AlexanderNixon/Machine-learning-reads/blob/master/Movie-content-based-recommender-using-tf-idf.ipynb\n",
    "https://ithelp.ithome.com.tw/articles/10220479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('./data/users.csv')\n",
    "subgroups_df = pd.read_csv('./data/subgroups.csv')\n",
    "course_chapter_items_df = pd.read_csv('./data/course_chapter_items.csv')\n",
    "course_df_ori = pd.read_csv('./data/courses.csv').fillna('')\n",
    "\n",
    "train_group_df = pd.read_csv('./data/train_group.csv')\n",
    "test_seen_group_df = pd.read_csv('./data/test_seen_group.csv')\n",
    "val_seen_group_df = pd.read_csv('./data/val_seen_group.csv')\n",
    "test_unseen_group_df = pd.read_csv('./data/test_unseen_group.csv')\n",
    "val_unseen_group_df = pd.read_csv('./data/val_unseen_group.csv')\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_seen_df = pd.read_csv('./data/test_seen.csv')\n",
    "val_seen_df = pd.read_csv('./data/val_seen.csv')\n",
    "test_unseen_df = pd.read_csv('./data/test_unseen.csv')\n",
    "val_unseen_df = pd.read_csv('./data/val_unseen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2course_mapping = course_df_ori[\"course_id\"].to_dict()\n",
    "course2id_mapping = {v : k for k, v in id2course_mapping.items()}\n",
    "len(course2id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df = course_df_ori.drop(['teacher_id', 'course_published_at_local', 'course_published_at_local', 'course_price'], axis=1)\n",
    "for i in range(len(course_df)):\n",
    "    s = ''\n",
    "    s = course_df['description'][i]\n",
    "    s1 = BeautifulSoup(s ,'html').text\n",
    "    course_df['description'][i] = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df = course_df.replace('\\n', '',regex=True).replace('&.;', '',regex=True).replace(\"--&?\", \"\",regex=True).replace(\"\\t\", \"\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>course_name</th>\n",
       "      <th>teacher_intro</th>\n",
       "      <th>groups</th>\n",
       "      <th>sub_groups</th>\n",
       "      <th>groups+subgroups</th>\n",
       "      <th>topics</th>\n",
       "      <th>description</th>\n",
       "      <th>will_learn</th>\n",
       "      <th>required_tools</th>\n",
       "      <th>recommended_background</th>\n",
       "      <th>target_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [course_id, course_name, teacher_intro, groups, sub_groups, groups+subgroups, topics, description, will_learn, required_tools, recommended_background, target_group]\n",
       "Index: []"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>teacher_intro</th>\n",
       "      <th>groups</th>\n",
       "      <th>sub_groups</th>\n",
       "      <th>groups+subgroups</th>\n",
       "      <th>topics</th>\n",
       "      <th>will_learn</th>\n",
       "      <th>target_group</th>\n",
       "      <th>description</th>\n",
       "      <th>recommended_background</th>\n",
       "      <th>required_tools</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>少女人妻華麗變身：七大妝容七彩的夢幻樂園</td>\n",
       "      <td>在美妝KOL圈裡屬個人風格強烈的Alice，在清新與叛逆風格間遊刃有餘，其幽默的美妝影片手法...</td>\n",
       "      <td>生活品味</td>\n",
       "      <td>更多生活品味,護膚保養與化妝</td>\n",
       "      <td>生活品味_更多生活品味,生活品味_護膚保養與化妝</td>\n",
       "      <td>更多生活品味,護膚保養與化妝</td>\n",
       "      <td>不再害怕各種顏色的彩妝，可以更隨心搭配各種繽紛的顏色。</td>\n",
       "      <td>熱愛彩妝的人</td>\n",
       "      <td>少女人妻第一堂線上課程，跟著我一起華麗變身！在社群上許多人經常問我：「為什麼人妻畫的妝那麼好...</td>\n",
       "      <td>只要你有一顆愛化妝、想變漂亮的心皆可以參加。⚠️雖然課程當中會帶到相關彩妝技巧，不過內容偏向...</td>\n",
       "      <td>所需工具為：視課程實際會用到的彩妝用品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>幾何圖形分割X色塊組合</td>\n",
       "      <td>從學生時代開始摸索photoshop等軟體，自以為有些天賦但後來發現其實沒有。出社會後從事美...</td>\n",
       "      <td>藝術,設計</td>\n",
       "      <td>平面設計,繪畫與插畫</td>\n",
       "      <td>設計_平面設計,藝術_繪畫與插畫</td>\n",
       "      <td>Illustrator/以拉,配色技巧</td>\n",
       "      <td>可以將任何區塊分割成自己想要的幾何圖形、快速的上色，並且能夠應用在許多地方</td>\n",
       "      <td>每一位興趣的人都能學得來，非常容易的小技巧</td>\n",
       "      <td>這是一個有趣的小技巧，主要利用illustrator裡面的\"分割“這個功能，我已經想不起來是...</td>\n",
       "      <td>知道如何使用Illustrator的基本工具列</td>\n",
       "      <td>AdobeIllustrator（必備）,camera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            course_name                                      teacher_intro  \\\n",
       "0  少女人妻華麗變身：七大妝容七彩的夢幻樂園  在美妝KOL圈裡屬個人風格強烈的Alice，在清新與叛逆風格間遊刃有餘，其幽默的美妝影片手法...   \n",
       "1           幾何圖形分割X色塊組合  從學生時代開始摸索photoshop等軟體，自以為有些天賦但後來發現其實沒有。出社會後從事美...   \n",
       "\n",
       "  groups      sub_groups          groups+subgroups               topics  \\\n",
       "0   生活品味  更多生活品味,護膚保養與化妝  生活品味_更多生活品味,生活品味_護膚保養與化妝       更多生活品味,護膚保養與化妝   \n",
       "1  藝術,設計      平面設計,繪畫與插畫          設計_平面設計,藝術_繪畫與插畫  Illustrator/以拉,配色技巧   \n",
       "\n",
       "                              will_learn           target_group  \\\n",
       "0            不再害怕各種顏色的彩妝，可以更隨心搭配各種繽紛的顏色。                 熱愛彩妝的人   \n",
       "1  可以將任何區塊分割成自己想要的幾何圖形、快速的上色，並且能夠應用在許多地方  每一位興趣的人都能學得來，非常容易的小技巧   \n",
       "\n",
       "                                         description  \\\n",
       "0  少女人妻第一堂線上課程，跟著我一起華麗變身！在社群上許多人經常問我：「為什麼人妻畫的妝那麼好...   \n",
       "1  這是一個有趣的小技巧，主要利用illustrator裡面的\"分割“這個功能，我已經想不起來是...   \n",
       "\n",
       "                              recommended_background  \\\n",
       "0  只要你有一顆愛化妝、想變漂亮的心皆可以參加。⚠️雖然課程當中會帶到相關彩妝技巧，不過內容偏向...   \n",
       "1                            知道如何使用Illustrator的基本工具列   \n",
       "\n",
       "                required_tools  \n",
       "0          所需工具為：視課程實際會用到的彩妝用品  \n",
       "1  AdobeIllustrator（必備）,camera  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(x):\n",
    "        return x.replace(\" \", \"\")\n",
    "\n",
    "fillna=course_df.fillna('')\n",
    "features= ['course_name', 'teacher_intro', 'groups', 'sub_groups', 'groups+subgroups', 'topics', 'will_learn', 'target_group', 'description', 'recommended_background', 'required_tools']\n",
    "fillna=fillna[features]\n",
    "for feature in features:\n",
    "    fillna[feature] = fillna[feature].apply(clean_data)\n",
    "    \n",
    "fillna.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chinese_keybert import Chinese_Extractor\n",
    "kw_extractor = Chinese_Extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna['kw_name'] = ''\n",
    "fillna['kw_intro'] = ''\n",
    "fillna['kw_topics'] = ''\n",
    "fillna['kw_will'] = ''\n",
    "fillna['kw_target'] = ''\n",
    "fillna['kw_desc'] = ''\n",
    "fillna['kw_recommend'] = ''\n",
    "fillna['kw_tool'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2970.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6482.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13400.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.34it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7839.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7913.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6250.83it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.08it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3685.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5223.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9822.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10565.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8371.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7854.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3751.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10155.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2077.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3160.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9362.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14716.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6150.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4634.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.33it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7752.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1028.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2294.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3653.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7767.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.57it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1507.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10922.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8665.92it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3983.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3634.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8594.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3084.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 57.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8081.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 63.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9218.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9576.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7612.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1258.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2499.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11814.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 63.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13662.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11683.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 60.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4599.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11683.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 58.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10459.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5023.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4048.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7710.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8507.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9238.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2886.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3698.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3741.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10565.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5282.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9098.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4946.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4644.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 13.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11037.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2449.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12336.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4554.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3894.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8981.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14926.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12264.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12748.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10672.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5629.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6754.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10230.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9157.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7219.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10645.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9754.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2777.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4500.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6657.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14217.98it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 60.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11814.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 61.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4843.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6355.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11949.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.94it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7371.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5242.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6278.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10951.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7825.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.40it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9776.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7231.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1114.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2608.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2959.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11781.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 54.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7061.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6553.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8943.08it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 58.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10255.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 40.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8097.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5866.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 16384.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 59.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12557.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4228.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 54.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6605.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 60.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 60.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10356.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 58.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2334.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 46.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7869.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6732.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5629.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1919.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 57.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10699.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3084.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 65.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5533.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6278.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8371.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.21it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 23.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12905.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11244.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7557.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13066.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 62.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11848.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 65.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9597.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12087.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5295.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3498.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10280.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8981.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8525.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7584.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10672.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10810.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10330.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6132.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11459.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10255.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6423.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5197.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6967.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11214.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 58.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 58.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5637.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5197.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10230.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7410.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7612.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2528.21it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8612.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1680.41it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2133.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6482.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.35it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10672.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2549.73it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12018.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6553.60it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 59.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9258.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10951.19it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10699.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9177.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9845.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11650.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4578.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4882.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1823.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8490.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 21.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7133.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.48it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14873.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4854.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12300.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11618.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6241.52it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.68it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10180.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7096.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13797.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1961.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2298.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8756.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 18396.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9554.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6605.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9962.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 46.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6355.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7332.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5475.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11949.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 54.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10754.63it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12865.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.77it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7796.10it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 60.38it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14027.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9000.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10034.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6626.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5849.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.20it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.13it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4766.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10810.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7724.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.17it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9754.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10131.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 836.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1772.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3226.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11748.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5059.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2595.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3294.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12905.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7371.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.02it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2208.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 57.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12157.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12826.62it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7002.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5035.18it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14315.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 54.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9218.25it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11037.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7738.57it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3823.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4355.46it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8648.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6052.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7319.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.03it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4438.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10255.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4882.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11305.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.92it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9058.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6423.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8004.40it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.87it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8719.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11366.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10894.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7653.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.60it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12087.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.24it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11949.58it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 54.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3724.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.96it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7653.84it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5171.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6335.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9510.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9731.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4194.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7989.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 50.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5629.94it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4249.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 61.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4223.87it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 59.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11275.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10618.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.86it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7371.36it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 58.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8305.55it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 19.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10591.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15252.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 65.31it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13706.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 66.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11522.81it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 46.74it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12557.80it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 61.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9939.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.50it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.00it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6069.90it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5171.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 59.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15196.75it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 52.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11008.67it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4332.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5983.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 24.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11335.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 57.81it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2666.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.44it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7695.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10979.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 46.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 15307.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11554.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10155.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 39.76it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.41it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6034.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5737.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2322.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10782.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8176.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.55it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11915.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4993.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6887.20it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 15.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10255.02it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 38.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7332.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 36.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5384.22it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5974.79it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9020.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 30.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 612.49it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.21s/it]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1551.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8924.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8240.28it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4476.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.93it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9576.04it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12710.01it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 62.80it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4728.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4415.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10837.99it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.32it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8848.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 57.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6260.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.91it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3771.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3612.66it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7854.50it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4928.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 35.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10538.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 33.37it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9489.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.47it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7269.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5769.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13934.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 64.16it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14614.30it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 65.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11066.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 14873.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.70it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13066.37it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.71it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 3862.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 62.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4888.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8507.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.75it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2216.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8886.24it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 22.90it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5983.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 57.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10305.42it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.72it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6978.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 2974.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4568.96it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9383.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6636.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 54.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9915.61it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.59it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10727.12it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.39it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5041.23it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4744.69it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 16.49it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8473.34it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 46.54it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7436.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.95it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8507.72it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.53it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7194.35it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6512.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 20.69it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6114.15it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.78it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6875.91it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10010.27it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.56it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11586.48it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6052.39it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4505.16it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10106.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 53.28it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12087.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.63it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9118.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 28.07it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6543.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.36it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11459.85it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 10407.70it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11037.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6898.53it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.29it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5190.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6864.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 14.98it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4928.68it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 56.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7543.71it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 55.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9532.51it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1886.78it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 49.64it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6168.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5084.00it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13189.64it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.12it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 11155.06it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 44.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12228.29it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 47.46it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7231.56it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 48.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 37.82it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9279.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 41.88it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6700.17it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 51.84it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 5059.47it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.89it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1897.88it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8738.13it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 27.18it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 17924.38it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 34.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9279.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 42.15it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6123.07it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 43.09it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 6017.65it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.26it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 13751.82it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 31.19it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8683.86it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 45.58it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 17.11it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8208.03it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 8594.89it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 26.62it/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 687.37it/s]\n",
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(len(fillna)):\n",
    "    s1_name = ''\n",
    "    s1_intro = ''\n",
    "    s1_topics = ''\n",
    "    s1_will = ''\n",
    "    s1_target = ''\n",
    "    s1_desc = ''\n",
    "    s1_recommend = ''\n",
    "    s1_tool = ''\n",
    "    s_name = fillna['course_name'][i]\n",
    "    s_intro = fillna['teacher_intro'][i]\n",
    "    s_topics = fillna['topics'][i]\n",
    "    s_will = fillna['will_learn'][i]\n",
    "    s_target = fillna['target_group'][i]\n",
    "    s_desc = fillna['description'][i]\n",
    "    s_recommend = fillna['recommended_background'][i]\n",
    "    s_tool = fillna['required_tools'][i]\n",
    "    print(i)\n",
    "    try:\n",
    "        try:\n",
    "            kw_name = kw_extractor.generate_keywords(s_name,top_k=3,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for j in range(3):\n",
    "                s1_name = s1_name + kw_name[0][j] + ','\n",
    "            fillna['kw_name'][i] = s1_name\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_intro = kw_extractor.generate_keywords(s_intro,top_k=5,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for k in range(5):\n",
    "                s1_intro = s1_intro + kw_intro[0][k] + ','\n",
    "            fillna['kw_intro'][i] = s1_intro\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_topics = kw_extractor.generate_keywords(s_topics,top_k=3,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for j in range(3):\n",
    "                s1_topics = s1_topics + kw_topics[0][j] + ','\n",
    "            fillna['kw_topics'][i] = s1_topics\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_will = kw_extractor.generate_keywords(s_will,top_k=3,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for j in range(3):\n",
    "                s1_will = s1_will + kw_will[0][j] + ','\n",
    "            fillna['kw_will'][i] = s1_will\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_target = kw_extractor.generate_keywords(s_target,top_k=2,rank_methods=\"mmr\",diversity=0.1)\n",
    "            s1_target = s1_target + kw_target[0][0] + ','\n",
    "            fillna['kw_target'][i] = s1_target\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_desc = kw_extractor.generate_keywords(s_desc,top_k=5,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for k in range(5):\n",
    "                s1_desc = s1_desc + kw_desc[0][k] + ','\n",
    "            fillna['kw_desc'][i] = s1_desc\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_recommend = kw_extractor.generate_keywords(s_recommend,top_k=2,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for l in range(2):\n",
    "                s1_recommend = s1_recommend + kw_recommend[0][l] + ','\n",
    "            fillna['kw_recommend'][i] = s1_recommend\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            kw_tool = kw_extractor.generate_keywords(s_tool,top_k=3,rank_methods=\"mmr\",diversity=0.15)\n",
    "            for l in range(2):\n",
    "                s1_tool = s1_tool + kw_tool[0][l] + ','\n",
    "            fillna['kw_tool'][i] = s1_tool\n",
    "        except:\n",
    "            continue\n",
    "        #print(s1)   \n",
    "    except:\n",
    "        continue\n",
    "fillna['kw_name'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'發音,基礎,初學,英文,改善,'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fillna['kw_recommend'][145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination(x):\n",
    "    return x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['topics'] + ' ' + x['will_learn'] + ' ' + x['target_group'] + ' ' + x['description'] + ' ' + x['recommended_background']\n",
    "def create_combination1(x):\n",
    "    course_name = x['course_name']\n",
    "    teacher_intro = x['teacher_intro']\n",
    "    will_learn = x['will_learn']\n",
    "    recommended_background = x['recommended_background']\n",
    "    target_group = x['target_group']\n",
    "    required_tools = x['required_tools']\n",
    "    group = x['groups']\n",
    "    #text = course_name + ' ' + teacher_intro +  ' ' +  group + ' ' + group + ' ' + group + ' ' + x['sub_groups'] +' ' + x['sub_groups'] + ' ' +x['sub_groups'] + ' ' +  x['topics'] + ' ' + will_learn + ' ' + x['groups+subgroups'] + ' ' + teacher_intro\n",
    "    text = x['kw_target'] +' ' + x['kw_target'] +' ' + x['kw_tool'] +' ' + x['kw_tool'] +' ' + x['kw_intro'] +' ' + x['kw_name'] +' ' + x['kw_name'] +' ' + x['kw_name'] + ' ' + x['kw_desc']+ ' ' + x['kw_desc']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['target_group'] + ' ' + x['recommended_background'] + ' ' +x['recommended_background'] + ' ' + x['groups+subgroups']\n",
    "    #text = course_name + ' ' + teacher_intro + ' ' + x['groups'] + ' ' + x['sub_groups'] + ' ' + x['topics'] + ' ' + will_learn + ' ' + recommended_background + ' ' + target_group + ' ' + x['groups+subgroups']\n",
    "\n",
    "    return text\n",
    "fillna['combination'] = fillna.apply(create_combination1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_name</th>\n",
       "      <th>teacher_intro</th>\n",
       "      <th>groups</th>\n",
       "      <th>sub_groups</th>\n",
       "      <th>groups+subgroups</th>\n",
       "      <th>topics</th>\n",
       "      <th>will_learn</th>\n",
       "      <th>target_group</th>\n",
       "      <th>description</th>\n",
       "      <th>recommended_background</th>\n",
       "      <th>required_tools</th>\n",
       "      <th>kw_desc</th>\n",
       "      <th>combination</th>\n",
       "      <th>combination2</th>\n",
       "      <th>kw_name</th>\n",
       "      <th>kw_intro</th>\n",
       "      <th>kw_recommend</th>\n",
       "      <th>kw_tool</th>\n",
       "      <th>kw_gs</th>\n",
       "      <th>kw_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>少女人妻華麗變身：七大妝容七彩的夢幻樂園</td>\n",
       "      <td>在美妝KOL圈裡屬個人風格強烈的Alice，在清新與叛逆風格間遊刃有餘，其幽默的美妝影片手法...</td>\n",
       "      <td>生活品味</td>\n",
       "      <td>更多生活品味,護膚保養與化妝</td>\n",
       "      <td>生活品味_更多生活品味,生活品味_護膚保養與化妝</td>\n",
       "      <td>更多生活品味,護膚保養與化妝</td>\n",
       "      <td></td>\n",
       "      <td>熱愛彩妝的人</td>\n",
       "      <td>少女人妻第一堂線上課程，跟著我一起華麗變身！在社群上許多人經常問我：「為什麼人妻畫的妝那麼好...</td>\n",
       "      <td>只要你有一顆愛化妝、想變漂亮的心皆可以參加。⚠️雖然課程當中會帶到相關彩妝技巧，不過內容偏向...</td>\n",
       "      <td>所需工具為：視課程實際會用到的彩妝用品</td>\n",
       "      <td>彩妝,少女人妻,美妝,畫出,眼妝,</td>\n",
       "      <td>彩妝,課程,工具, 彩妝,課程,工具, 美妝,幽默,粉絲,風格, 妝容,夢幻,七彩, 妝...</td>\n",
       "      <td>在美妝KOL圈裡屬個人風格強烈的Alice，在清新與叛逆風格間遊刃有餘，其幽默的美妝影片手法...</td>\n",
       "      <td>妝容,夢幻,七彩,</td>\n",
       "      <td>美妝,幽默,粉絲,風格,</td>\n",
       "      <td>彩妝,參加,課程,色系,漂亮,</td>\n",
       "      <td>彩妝,課程,工具,</td>\n",
       "      <td>生活,保養,品味,</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>幾何圖形分割X色塊組合</td>\n",
       "      <td>從學生時代開始摸索photoshop等軟體，自以為有些天賦但後來發現其實沒有。出社會後從事美...</td>\n",
       "      <td>藝術,設計</td>\n",
       "      <td>平面設計,繪畫與插畫</td>\n",
       "      <td>設計_平面設計,藝術_繪畫與插畫</td>\n",
       "      <td>Illustrator/以拉,配色技巧</td>\n",
       "      <td></td>\n",
       "      <td>每一位興趣的人都能學得來，非常容易的小技巧</td>\n",
       "      <td>這是一個有趣的小技巧，主要利用illustrator裡面的\"分割“這個功能，我已經想不起來是...</td>\n",
       "      <td>知道如何使用Illustrator的基本工具列</td>\n",
       "      <td>AdobeIllustrator（必備）,camera</td>\n",
       "      <td>畫畫,分割,技巧,油漆桶,幾何,</td>\n",
       "      <td>畫畫,設計,印刷,美術, 幾何,分割,圖形, 幾何,分割,圖形, 幾何,分割,圖形,...</td>\n",
       "      <td>從學生時代開始摸索photoshop等軟體，自以為有些天賦但後來發現其實沒有。出社會後從事美...</td>\n",
       "      <td>幾何,分割,圖形,</td>\n",
       "      <td>畫畫,設計,印刷,美術,</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>繪畫,設計,藝術,</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            course_name                                      teacher_intro  \\\n",
       "0  少女人妻華麗變身：七大妝容七彩的夢幻樂園  在美妝KOL圈裡屬個人風格強烈的Alice，在清新與叛逆風格間遊刃有餘，其幽默的美妝影片手法...   \n",
       "1           幾何圖形分割X色塊組合  從學生時代開始摸索photoshop等軟體，自以為有些天賦但後來發現其實沒有。出社會後從事美...   \n",
       "\n",
       "  groups      sub_groups          groups+subgroups               topics  \\\n",
       "0   生活品味  更多生活品味,護膚保養與化妝  生活品味_更多生活品味,生活品味_護膚保養與化妝       更多生活品味,護膚保養與化妝   \n",
       "1  藝術,設計      平面設計,繪畫與插畫          設計_平面設計,藝術_繪畫與插畫  Illustrator/以拉,配色技巧   \n",
       "\n",
       "  will_learn           target_group  \\\n",
       "0                            熱愛彩妝的人   \n",
       "1             每一位興趣的人都能學得來，非常容易的小技巧   \n",
       "\n",
       "                                         description  \\\n",
       "0  少女人妻第一堂線上課程，跟著我一起華麗變身！在社群上許多人經常問我：「為什麼人妻畫的妝那麼好...   \n",
       "1  這是一個有趣的小技巧，主要利用illustrator裡面的\"分割“這個功能，我已經想不起來是...   \n",
       "\n",
       "                              recommended_background  \\\n",
       "0  只要你有一顆愛化妝、想變漂亮的心皆可以參加。⚠️雖然課程當中會帶到相關彩妝技巧，不過內容偏向...   \n",
       "1                            知道如何使用Illustrator的基本工具列   \n",
       "\n",
       "                required_tools            kw_desc  \\\n",
       "0          所需工具為：視課程實際會用到的彩妝用品  彩妝,少女人妻,美妝,畫出,眼妝,   \n",
       "1  AdobeIllustrator（必備）,camera   畫畫,分割,技巧,油漆桶,幾何,   \n",
       "\n",
       "                                         combination  \\\n",
       "0    彩妝,課程,工具, 彩妝,課程,工具, 美妝,幽默,粉絲,風格, 妝容,夢幻,七彩, 妝...   \n",
       "1      畫畫,設計,印刷,美術, 幾何,分割,圖形, 幾何,分割,圖形, 幾何,分割,圖形,...   \n",
       "\n",
       "                                        combination2    kw_name      kw_intro  \\\n",
       "0  在美妝KOL圈裡屬個人風格強烈的Alice，在清新與叛逆風格間遊刃有餘，其幽默的美妝影片手法...  妝容,夢幻,七彩,  美妝,幽默,粉絲,風格,   \n",
       "1  從學生時代開始摸索photoshop等軟體，自以為有些天賦但後來發現其實沒有。出社會後從事美...  幾何,分割,圖形,  畫畫,設計,印刷,美術,   \n",
       "\n",
       "      kw_recommend    kw_tool      kw_gs kw_target  \n",
       "0  彩妝,參加,課程,色系,漂亮,  彩妝,課程,工具,  生活,保養,品味,            \n",
       "1                              繪畫,設計,藝術,            "
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fillna.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    #without_duplicates = re.sub(r'(.)\\1+', r'\\1\\1', sentence)\n",
    "    without_duplicates = sentence\n",
    "    bd='*_▌~�❗\\'⃣✅▐➤✨❤▒▍–∣⭐'\n",
    "    for i in bd:\n",
    "        without_duplicates=without_duplicates.replace(i,'')\n",
    "    #ws_token = ws(without_duplicates)\n",
    "    return jieba.lcut(without_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=jieba.lcut, stop_words='english')\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(fillna['combination'])\n",
    "tfidf_matrix_nd = tfidf_matrix.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combination2(x):\n",
    "    course_name = x['course_name']\n",
    "    teacher_intro = x['teacher_intro']\n",
    "    will_learn = x['will_learn']\n",
    "    recommended_background = x['recommended_background']\n",
    "    target_group = x['target_group']\n",
    "    required_tools = x['required_tools']\n",
    "    group = x['groups']\n",
    "    text = teacher_intro +  ' ' +  group + ' ' + group +' ' + x['sub_groups'] + ' ' +x['sub_groups'] + ' ' +  x['topics'] + ' ' + x['groups+subgroups'] + ' ' + teacher_intro  + ' ' + x['groups+subgroups'] + ' ' + x['groups+subgroups'] + ' ' + x['groups+subgroups']\n",
    "    #text = course_name + ' ' + teacher_intro + ' ' + x['groups'] + ' ' + x['sub_groups'] + ' ' + x['topics'] + ' ' + will_learn + ' ' + recommended_background + ' ' + target_group + ' ' + x['groups+subgroups']\n",
    "\n",
    "    return text\n",
    "fillna['combination2'] = fillna.apply(create_combination2, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(tokenizer=jieba.lcut, stop_words='english')\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix2 = tfidf2.fit_transform(fillna['combination2'])\n",
    "tfidf_matrix_nd2 = tfidf_matrix2.toarray()\n",
    "np.save('tfidf_embedding/item_embeddings_jieba_english_course1.npy', tfidf_matrix_nd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(728, 15748)\n",
      "(728, 8630)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "item_embed_FM = np.load('tfidf_embedding/item_embeddings_jieba_english_course1.npy')\n",
    "item_embed_FM = item_embed_FM[:728]\n",
    "print(tfidf_matrix.shape)\n",
    "print(item_embed_FM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(728, 24378)\n"
     ]
    }
   ],
   "source": [
    "concatenate_item_embed_nd = np.concatenate([tfidf_matrix_nd, item_embed_FM/2], axis = 1)\n",
    "print(concatenate_item_embed_nd.shape)\n",
    "concatenate_item_embed = csr_matrix(concatenate_item_embed_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(728, 24378)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(728, 728)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "print(concatenate_item_embed_nd.shape)\n",
    "cosine_sim2 = linear_kernel(concatenate_item_embed, concatenate_item_embed)\n",
    "#Output the shape of tfidf_matrix\n",
    "cosine_sim2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5d5aac6e8e9eb60021211902',\n",
       " '6184efc3b2319400078aefe7',\n",
       " '56a1e0164ec4c609007d2cb0',\n",
       " '5f7c210b1de7982fb413a3e9',\n",
       " '5fe5f2025a899784aa96f52b',\n",
       " '5f7c209762ad22756c7a1c74',\n",
       " '5f7c212262ad2203e77a1cc9',\n",
       " '6155cda6d425f500065f5c96',\n",
       " '5ef616f7303ca82d6d08e2eb',\n",
       " '6125b83cdf147200070db995']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recommendations_new(haved_courses_list, cosine_sim, top = 50):\n",
    "    cosine_sim_sum = [ [i, 0] for i in range(len(course2id_mapping))]\n",
    "    haved_courses_index_list = [ course2id_mapping[course_id] for course_id in haved_courses_list]\n",
    "    for idx in haved_courses_index_list:\n",
    "        # Get the pairwsie similarity scores of all courses with that course\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        for i in range(len(sim_scores)):\n",
    "            cosine_sim_sum[i][1] += sim_scores[i][1]\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(cosine_sim_sum, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    recommend_indices = []\n",
    "    # Get the scores of the 10 most course not buy\n",
    "    for i in range(len(sim_scores)):\n",
    "        if len(recommend_indices) < top:\n",
    "            if sim_scores[i][0] not in haved_courses_index_list:\n",
    "                recommend_indices.append(sim_scores[i][0])\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return course_df['course_id'].iloc[recommend_indices].tolist()\n",
    "    \n",
    "get_recommendations_new(['5f194354cad0d086f3ee24cf', '6156a77fdf426a0007cc5fe1'], cosine_sim2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7205\n",
      "59737\n"
     ]
    }
   ],
   "source": [
    "seen_predict_df = pd.read_csv('./data/test_seen_group.csv')\n",
    "predict_user = seen_predict_df[\"user_id\"].to_list()\n",
    "print(len(predict_user))\n",
    "\n",
    "\n",
    "seen_user_haved_purchased_course = {}\n",
    "for seen_user_id, course_ids in zip(train_df[\"user_id\"], train_df[\"course_id\"]):\n",
    "    course_id_list = course_ids.split(' ')\n",
    "    seen_user_haved_purchased_course[seen_user_id] = seen_user_haved_purchased_course.setdefault(seen_user_id, []) + course_id_list\n",
    "for seen_user_id, course_ids in zip(val_seen_df[\"user_id\"], val_seen_df[\"course_id\"]):\n",
    "    course_id_list = course_ids.split(' ')\n",
    "    seen_user_haved_purchased_course[seen_user_id] = seen_user_haved_purchased_course.setdefault(seen_user_id, []) + course_id_list\n",
    "print(len(seen_user_haved_purchased_course))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "predict_users = seen_predict_df[\"user_id\"].to_list()\n",
    "with open(\"predict50_82434.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"user_id\", \"course_id\"])\n",
    "    for user_id in predict_users:\n",
    "        recommend = \" \".join(get_recommendations_new(seen_user_haved_purchased_course[user_id], cosine_sim2, top = 50))\n",
    "        writer.writerow([user_id, recommend])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('56dae2b74e3ef90900b7bd0e', ['5b61928a8011d1001e356102']),\n",
       " ('60e66f29be3e3b0006c4db75', ['559e49185850311000fca504']),\n",
       " ('5c919efb728ddf00208b9b2b',\n",
       "  ['60ddc3ca06259d00064c7f17',\n",
       "   '60aeac37bca91777bf5bb114',\n",
       "   '60c84de9eb75ca46e0c25e85',\n",
       "   '611f7d91bd122100071f2926',\n",
       "   '611f5d074b76af0007c24d7e',\n",
       "   '6155cda6d425f500065f5c96',\n",
       "   '6156a77fdf426a0007cc5fe1']),\n",
       " ('5ac115507997a2001e7c3617', ['58d5c70c27ea7d070060160e']),\n",
       " ('5f53b84440c5be3bb873a9d3', ['5f55fb39b34335d28416bd0c'])]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_user_haved_purchased_course = {}\n",
    "for seen_user_id, course_ids in zip(train_df[\"user_id\"], train_df[\"course_id\"]):\n",
    "    course_id_list = course_ids.split(' ')\n",
    "    train_user_haved_purchased_course[seen_user_id] = train_user_haved_purchased_course.setdefault(seen_user_id, []) + course_id_list\n",
    "\n",
    "val_answer_course = {}\n",
    "val_seen_df_fillna = val_seen_df.fillna(\"\")\n",
    "for seen_user_id, course_ids in zip(val_seen_df_fillna[\"user_id\"], val_seen_df_fillna[\"course_id\"]):\n",
    "    if len(course_ids) > 0:\n",
    "        course_id_list = [ str(x) for x in course_ids.split(' ')]\n",
    "        val_answer_course[seen_user_id] = val_answer_course.setdefault(seen_user_id, []) + course_id_list\n",
    "list(val_answer_course.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0824505022377611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0824505022377611"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ml_metrics\n",
    "\n",
    "answers = []\n",
    "predictions = []\n",
    "map50s = []\n",
    "for user_id in val_seen_df[\"user_id\"]:\n",
    "    prediction = get_recommendations_new(train_user_haved_purchased_course[user_id], cosine_sim2, top = 50)\n",
    "    #print(prediction)\n",
    "    prediction_idxs = prediction\n",
    "    if user_id in val_answer_course.keys():\n",
    "        answer_idxs = val_answer_course[user_id]\n",
    "    else:\n",
    "        answer_idxs = []\n",
    "        \n",
    "    # print(user_id)\n",
    "    # print([course2subgroups[x] for x in train_user_haved_purchased_course[user_id]])\n",
    "    # print(prediction) \n",
    "    # print(answer_idxs)\n",
    "    # print(\"\")\n",
    "\n",
    "    predictions.append(prediction_idxs)\n",
    "    answers.append(answer_idxs)\n",
    "    map50s.append(ml_metrics.mapk(predicted= [prediction_idxs], actual= [answer_idxs], k = 50))\n",
    "    \n",
    "print(np.mean(map50s))\n",
    "#answers.append(answers)        \n",
    "\n",
    "map50 = ml_metrics.mapk(predicted= predictions, actual= answers, k = 50)\n",
    "map50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x['course_name']+ ' ' + x['teacher_intro'] + ' ' + x['sub_groups'] + ' ' + x['topics'] + ' ' + x['will_learn'] + ' ' + x['target_group'] \n",
    "0.06871480941842756\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['topics'] + ' ' + x['will_learn'] + ' ' + x['target_group'] + ' ' + x['description'] + ' ' + x['recommended_background']\n",
    "0.05496563647510346\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['topics'] + ' ' + x['will_learn'] + ' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06712582130333207\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['topics'] + ' ' + x['will_learn'] + ' ' + x['target_group']\n",
    "0.06597312470076507\n",
    "\n",
    "x['course_name']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['topics'] + ' ' + x['will_learn'] + ' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.03404063813630374\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['topics'] + ' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.0676415508550833\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06939486811195406\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06958664864078341\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['sub_groups'] +' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06779119882844009\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['recommended_background']\n",
    "0.06682865004279648\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['target_group'] + ' ' + x['recommended_background'] + ' ' + x['sub_groups']\n",
    "0.06676136192484118\n",
    "\n",
    "x['course_name']+ ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06576964508987501\n",
    "\n",
    "x['teacher_intro'] + ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06960659471092882\n",
    "x['teacher_intro'] + ' ' + x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06627308178826925\n",
    "x['teacher_intro'] + ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['target_group'] + ' ' + x['recommended_background']+ ' ' + x['groups']\n",
    "0.06754423548672533\n",
    "x['teacher_intro']+ ' ' + x['groups'] + ' ' + x['sub_groups'] +' ' + x['groups+subgroups'] + ' ' + x['target_group'] + ' ' + x['recommended_background']\n",
    "0.06942449805121588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlhw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dedd2b11c6d65023ba78f004c00adcb97ea7737d51ff1d03c402b08b92e7cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
