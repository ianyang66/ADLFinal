{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import datetime\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "import csv\n",
    "import ml_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('../data/users.csv')\n",
    "subgroups_df = pd.read_csv('../data/subgroups.csv')\n",
    "course_df_original = pd.read_csv('../data/courses.csv')\n",
    "\n",
    "train_group_df = pd.read_csv('../data/train_group.csv')\n",
    "test_seen_group_df = pd.read_csv('../data/test_seen_group.csv')\n",
    "val_seen_group_df = pd.read_csv('../data/val_seen_group.csv')\n",
    "\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_seen_df = pd.read_csv('../data/test_seen.csv')\n",
    "val_seen_df = pd.read_csv('../data/val_seen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df = pd.read_csv('./combination1231.csv').fillna('')\n",
    "course_df.insert(0, 'course_id', course_df_original['course_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2529/2039743899.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  course_df['description'][i] = s1\n"
     ]
    }
   ],
   "source": [
    "for i in range(728):\n",
    "    s = ''\n",
    "    s = course_df['description'][i]\n",
    "    s1 = BeautifulSoup(str(s) ,'html').text\n",
    "    course_df['description'][i] = s1\n",
    "course_df = course_df.replace('\\n', '',regex=True).replace('&.;', '',regex=True).replace(\"--&?\", \"\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillna=course_df.fillna('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2course_mapping = course_df_original[\"course_id\"].to_dict()\n",
    "course2id_mapping = {v : k for k, v in id2course_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(name, n):\n",
    "    text=''\n",
    "    if n-1 > 0:\n",
    "        for i in range(n):\n",
    "            text = text + name + ' '\n",
    "    elif n-1 == 0:\n",
    "        text = text + name + ' '\n",
    "    return text\n",
    "def combination_count(x, cn,ti,g,sg,gsg,t,d,wl,rt,rbg,tg, kcn,kti,kg,ksg,kgsg,kt,kd,kwl,krt,krbg,ktg):\n",
    "    course_name = x['course_name']\n",
    "    teacher_intro = x['teacher_intro']\n",
    "    will_learn = x['will_learn']\n",
    "    recommended_background = x['recommended_background']\n",
    "    target_group = x['target_group']\n",
    "    required_tools = x['required_tools']\n",
    "    text = ''\n",
    "    text += combination(course_name, cn)\n",
    "    text += combination(teacher_intro, ti)\n",
    "    text += combination(x['groups'], g)\n",
    "    text += combination(x['sub_groups'], sg)\n",
    "    text += combination(x['groups+subgroups'], gsg)\n",
    "    text += combination(x['topics'], t)\n",
    "    text += combination(x['description'], d)\n",
    "    text += combination(x['will_learn'], wl)\n",
    "    text += combination(x['required_tools'], rt)\n",
    "    text += combination(x['recommended_background'], rbg)\n",
    "    text += combination(x['target_group'], tg)\n",
    "    text += combination(x['kw_name'], kcn)\n",
    "    text += combination(x['kw_intro'], kti)\n",
    "    text += combination(x['kw_group'], kg)\n",
    "    text += combination(x['kw_sub'], ksg)\n",
    "    text += combination(x['kw_gs'], kgsg)\n",
    "    text += combination(x['kw_topics'], kt)\n",
    "    text += combination(x['kw_desc'], kd)\n",
    "    text += combination(x['kw_will'], kwl)\n",
    "    text += combination(x['kw_tool'], krt)\n",
    "    text += combination(x['kw_recommend'], krbg)\n",
    "    text += combination(x['kw_target'], ktg)\n",
    "    return text\n",
    "\n",
    "def create_combination(x):\n",
    "    text=''\n",
    "    # Adjust weights\n",
    "    text = combination_count(x, 1,2,4,2,1,3,0,1,1,1,0 ,0,2,3,1,1,0,1,1,1,1,0)\n",
    "    return text\n",
    "\n",
    "fillna['combination'] = fillna.apply(create_combination, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [k.strip() for k in open('baidu_stopwords_plus.txt', encoding='utf8').readlines() if k.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ian/miniconda3/envs/adlhw3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['don'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=None,stop_words=stopwords)\n",
    "\n",
    "# Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(fillna['combination'])\n",
    "\n",
    "# TF-IDF Dicitonary\n",
    "tfidf_matrix_nd = tfidf_matrix.toarray()\n",
    "np.save('item_embeddings_all.npy', tfidf_matrix_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embed_FM = np.load('item_embeddings_all.npy')\n",
    "item_embed_FM = item_embed_FM[:728]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_item_embed_nd = np.concatenate([tfidf_matrix_nd, item_embed_FM/5], axis = 1)\n",
    "concatenate_item_embed = csr_matrix(concatenate_item_embed_nd)\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim2 = linear_kernel(concatenate_item_embed, concatenate_item_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# According to course similarity to Predict Subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups2idx = {}\n",
    "for (id, name) in zip(subgroups_df[\"subgroup_id\"], subgroups_df[\"subgroup_name\"]):\n",
    "    subgroups2idx.update({name: id})\n",
    "idx2subgroups = {v : k for k, v in subgroups2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "course2subgroups = {}\n",
    "for (course_id, sub_groups) in zip(course_df_original[\"course_id\"], course_df_original[\"sub_groups\"]):\n",
    "    try:\n",
    "        if pd.isnull(sub_groups):\n",
    "            course2subgroups.update({course_id: []})\n",
    "        else:\n",
    "            course2subgroups.update({course_id: [ subgroups2idx[sub_group] for sub_group in sub_groups.split(',')]})\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_recommend(haved_courses_list, cosine_sim, top = 50):\n",
    "    cosine_sim_sum = [ [i, 0] for i in range(len(course2id_mapping))]\n",
    "    haved_courses_index_list = [ course2id_mapping[course_id] for course_id in haved_courses_list]\n",
    "    for idx in haved_courses_index_list:\n",
    "        # Get the pairwsie similarity scores of all courses with that course\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        for i in range(len(sim_scores)):\n",
    "            cosine_sim_sum[i][1] += sim_scores[i][1]\n",
    "\n",
    "    # Sort the similarity scores\n",
    "    sim_scores = sorted(cosine_sim_sum, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    recommend_subgroups = []\n",
    "    # Get the scores of the top 50 course not buy and tranform to subgroup\n",
    "    for i in range(len(sim_scores)):\n",
    "        if len(recommend_subgroups) < top:\n",
    "            if sim_scores[i][0] not in haved_courses_index_list:\n",
    "                course_id = course_df_original['course_id'][sim_scores[i][0]]\n",
    "                try:\n",
    "                    for subgroup in course2subgroups[course_id]:\n",
    "                        if subgroup not in recommend_subgroups:\n",
    "                            recommend_subgroups.append(subgroup)\n",
    "                except:\n",
    "                    continue\n",
    "    # Return the top 50 most similar subgroups\n",
    "    return recommend_subgroups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Accuracy of our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_haved_purchased_course = {}\n",
    "for seen_user_id, course_ids in zip(train_df[\"user_id\"], train_df[\"course_id\"]):\n",
    "    course_id_list = course_ids.split(' ')\n",
    "    train_haved_purchased_course[seen_user_id] = train_haved_purchased_course.setdefault(seen_user_id, []) + course_id_list\n",
    "\n",
    "val_subgroups = {}\n",
    "val_seen_group_df_fillna = val_seen_group_df.fillna(\"\")\n",
    "for seen_user_id, subgroup_ids in zip(val_seen_group_df_fillna[\"user_id\"], val_seen_group_df_fillna[\"subgroup\"]):\n",
    "    if len(subgroup_ids) > 0:\n",
    "        course_id_list = [ int(x) for x in subgroup_ids.split(' ')]\n",
    "        val_subgroups[seen_user_id] = val_subgroups.setdefault(seen_user_id, []) + course_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.274810096879608\n"
     ]
    }
   ],
   "source": [
    "answers, predictions, map50s = [], [], []\n",
    "for user_id in val_seen_group_df[\"user_id\"]:\n",
    "    prediction = get_new_recommend(train_haved_purchased_course[user_id], cosine_sim2, top = 50)\n",
    "\n",
    "    prediction_idxs = prediction\n",
    "    if user_id in val_subgroups.keys():\n",
    "        answer_idxs = val_subgroups[user_id]\n",
    "    else:\n",
    "        answer_idxs = []\n",
    "\n",
    "    predictions.append(prediction_idxs)\n",
    "    answers.append(answer_idxs)\n",
    "    map50s.append(ml_metrics.mapk(predicted= [prediction_idxs], actual= [answer_idxs], k = 50))\n",
    "    \n",
    "print(np.mean(map50s))      \n",
    "map50 = ml_metrics.mapk(predicted= predictions, actual= answers, k = 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_user = test_seen_group_df[\"user_id\"].to_list()\n",
    "\n",
    "seen_haved_purchased_course = {}\n",
    "for seen_user_id, course_ids in zip(train_df[\"user_id\"], train_df[\"course_id\"]):\n",
    "    course_id_list = course_ids.split(' ')\n",
    "    seen_haved_purchased_course[seen_user_id] = seen_haved_purchased_course.setdefault(seen_user_id, []) + course_id_list\n",
    "for seen_user_id, course_ids in zip(val_seen_df[\"user_id\"], val_seen_df[\"course_id\"]):\n",
    "    course_id_list = course_ids.split(' ')\n",
    "    seen_haved_purchased_course[seen_user_id] = seen_haved_purchased_course.setdefault(seen_user_id, []) + course_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_users = test_seen_group_df[\"user_id\"].to_list()\n",
    "with open(\"subgroup_predict50_without_jieba_baidu_stop.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"user_id\", \"subgroup\"])\n",
    "    for user_id in predict_users:\n",
    "        recommend_subgroups_str = [ str(x) for x in get_new_recommend(seen_haved_purchased_course[user_id], cosine_sim2, top = 50)]\n",
    "        recommend = \" \".join(recommend_subgroups_str)\n",
    "        writer.writerow([user_id, recommend])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adlhw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9dedd2b11c6d65023ba78f004c00adcb97ea7737d51ff1d03c402b08b92e7cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
